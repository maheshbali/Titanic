{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: FutureWarning: '.reindex_axis' is deprecated and will be removed in a future version. Use '.reindex' instead.\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:53: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# This script is forked from \"XGBoost example (Python)\" by DataCanary\n",
    "# https://www.kaggle.com/datacanary/xgboost-example-python?scriptVersionId=108683\n",
    "# here we used probability distribution for Ages instead of using mean or median\n",
    "# because there are 263 missed values and filling them with list of values may increase accuracy\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('titanic_train.csv', header=0)\n",
    "test_df = pd.read_csv('titanic_test.csv', header=0)\n",
    "\n",
    "feature_columns_to_use = ['Pclass','Sex','Age','Fare','Parch']\n",
    "\n",
    "# Join the features from train and test together before imputing missing values,\n",
    "# in case their distribution is slightly different\n",
    "big_X = train_df[feature_columns_to_use].append(test_df[feature_columns_to_use])\n",
    "\n",
    "# Fare column is just one value empty so we will fill it with median value\n",
    "big_X['Fare'] = big_X['Fare'].fillna(big_X['Fare'].median())\n",
    "\n",
    "# Creating probabilty distribution for ages from the existing column values\n",
    "ages_probabilities = big_X['Age'].value_counts().to_frame()\n",
    "ages_probabilities['index1'] = ages_probabilities.index\n",
    "ages_probabilities = ages_probabilities.rename(columns={'Age': 'Count', 'index1': 'Age'})\n",
    "ages_probabilities = ages_probabilities.reindex_axis(['Age','Count'], axis=1)\n",
    "ages_probabilities = ages_probabilities.reset_index()\n",
    "ages_probabilities = ages_probabilities.drop([\"index\"],axis=1)\n",
    "ages_probabilities['Probability'] = ages_probabilities['Count'] / big_X['Age'].value_counts().sum()\n",
    "\n",
    "input_ages_list = ages_probabilities['Age'].values.tolist()\n",
    "props_ages_list = ages_probabilities['Probability'].values.tolist()\n",
    "newAges = np.random.choice(input_ages_list, big_X['Age'].isnull().sum(), props_ages_list)\n",
    "\n",
    "# fill Ages null values with this distribution\n",
    "AgeNulls = big_X[pd.isnull(big_X['Age'])]\n",
    "for i, ni in enumerate(AgeNulls.index[:len(newAges)]):\n",
    "    big_X['Age'].loc[ni] = newAges[i]\n",
    "\n",
    "big_X_imputed = big_X\n",
    "\n",
    "# XGBoost doesn't (yet) handle categorical features automatically, so we need to change\n",
    "# them to columns of integer values.\n",
    "# See http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing for more\n",
    "# details and options\n",
    "le = LabelEncoder()\n",
    "big_X_imputed['Sex'] = le.fit_transform(big_X_imputed['Sex'])\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "train_X = big_X_imputed[0:train_df.shape[0]].as_matrix()\n",
    "test_X = big_X_imputed[train_df.shape[0]::].as_matrix()\n",
    "train_y = train_df['Survived']\n",
    "\n",
    "# You can experiment with many other options here, using the same .fit() and .predict()\n",
    "# methods; see http://scikit-learn.org\n",
    "# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(train_X, train_y)\n",
    "predictions = gbm.predict(test_X)\n",
    "\n",
    "# Kaggle needs the submission to have a certain format;\n",
    "# see https://www.kaggle.com/c/titanic-gettingStarted/download/gendermodel.csv\n",
    "# for an example of what it's supposed to look like.\n",
    "submission = pd.DataFrame({ 'PassengerId': test_df['PassengerId'],\n",
    "                            'Survived': predictions })\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         1\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "5            897         0\n",
      "6            898         1\n",
      "7            899         0\n",
      "8            900         1\n",
      "9            901         0\n",
      "10           902         0\n",
      "11           903         0\n",
      "12           904         1\n",
      "13           905         0\n",
      "14           906         1\n",
      "15           907         1\n",
      "16           908         0\n",
      "17           909         0\n",
      "18           910         1\n",
      "19           911         1\n",
      "20           912         0\n",
      "21           913         0\n",
      "22           914         1\n",
      "23           915         0\n",
      "24           916         1\n",
      "25           917         0\n",
      "26           918         1\n",
      "27           919         0\n",
      "28           920         0\n",
      "29           921         0\n",
      "..           ...       ...\n",
      "388         1280         0\n",
      "389         1281         0\n",
      "390         1282         1\n",
      "391         1283         1\n",
      "392         1284         0\n",
      "393         1285         0\n",
      "394         1286         0\n",
      "395         1287         1\n",
      "396         1288         0\n",
      "397         1289         1\n",
      "398         1290         0\n",
      "399         1291         0\n",
      "400         1292         1\n",
      "401         1293         0\n",
      "402         1294         1\n",
      "403         1295         0\n",
      "404         1296         0\n",
      "405         1297         0\n",
      "406         1298         0\n",
      "407         1299         0\n",
      "408         1300         1\n",
      "409         1301         1\n",
      "410         1302         1\n",
      "411         1303         1\n",
      "412         1304         1\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
